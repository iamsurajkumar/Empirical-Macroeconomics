# Double Descent Demystified: Complete Tutorial

This repository contains a comprehensive tutorial on the **double descent phenomenon** in machine learning, based on the ICLR 2024 blog post ["Double Descent Demystified"](https://iclr-blogposts.github.io/2024/blog/double-descent-demystified/).

## ğŸ“š Contents

1. **`double_descent_tutorial.tex`** - Comprehensive LaTeX document with:
   - Detailed mathematical derivations
   - Clear explanations of all concepts
   - **Funny and absurd examples** to make the math memorable
   - Geometric intuitions
   - Step-by-step proofs

2. **`DoubleDescent.jl`** - Complete Julia implementation with:
   - Linear regression in both underparameterized and overparameterized regimes
   - SVD-based analysis
   - All three ablation experiments
   - Adversarial example generation
   - Visualization tools

3. **`run_experiments.jl`** - Script to replicate all experiments from the blog post

## ğŸš€ Quick Start

### Prerequisites

- Julia 1.6 or higher
- LaTeX distribution (for compiling the PDF)

### Installation

1. **Install Julia packages:**

```bash
julia --project=. -e 'using Pkg; Pkg.instantiate()'
```

Or manually:

```julia
julia> using Pkg
julia> Pkg.add(["Plots", "DataFrames", "CSV", "Downloads"])
```

2. **Compile the LaTeX document:**

```bash
pdflatex double_descent_tutorial.tex
pdflatex double_descent_tutorial.tex  # Run twice for references
```

Or use your favorite LaTeX editor (TeXShop, Overleaf, etc.)

### Running Experiments

```bash
julia run_experiments.jl
```

This will:
- Run double descent experiments on 4 datasets
- Perform all 3 ablation studies
- Generate adversarial examples
- Save all plots to `double_descent_results/`

Expected runtime: 5-10 minutes

## ğŸ“– Understanding Double Descent

### What is Double Descent?

Classical machine learning wisdom says: more model complexity â†’ worse generalization (after a point). But modern ML shows: make the model *even more* complex â†’ generalization improves again!

This creates a "double descent" curve:

```
Test Error
    â”‚     Classical U-shape
    â”‚          â†“
    â”‚    â•±â”€â”€â”€â”€â•²
    â”‚   â•±      â•²
    â”‚  â•±        â•²        Second descent!
    â”‚ â•±          â•²â•±â”€â”€â”€â”€â”€â”€
    â”‚â•±            â†‘
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Model Complexity
              Interpolation
              Threshold (N=P)
```

### The Three Factors

Double descent requires **ALL THREE** factors to be present simultaneously:

#### Factor 1: Small Singular Values (1/Ïƒáµ£)
- Training data has directions with very little variance
- Happens at interpolation threshold (N = D)
- Small Ïƒáµ£ â†’ huge 1/Ïƒáµ£ â†’ amplified errors!

#### Factor 2: Test Features in Weak Directions (x_test Â· váµ£)
- Test points project onto the weak directions
- Forces model to extrapolate where it has little information

#### Factor 3: Residual Errors (uáµ£ Â· E)
- Even the best model makes errors
- Can come from: noise, model misspecification, missing features
- These errors get amplified by 1/Ïƒáµ£

### The Critical Equation

The prediction error decomposes as:

```
Error = Î£áµ£ (1/Ïƒáµ£) Â· (x_test Â· váµ£) Â· (uáµ£ Â· E)
         ï¸¸â”â”â”â”ï¸¸   ï¸¸â”â”â”â”â”â”â”â”â”ï¸¸   ï¸¸â”â”â”â”â”ï¸¸
        Factor 1    Factor 2     Factor 3
```

Remove **any one** factor â†’ divergence disappears!

## ğŸ§ª Experiments Included

### 1. Basic Double Descent
- **Datasets:** Student-Teacher, California Housing, Diabetes, WHO Life Expectancy
- **Shows:** Test error spike at N = D

### 2. Factor 1 Ablation: Singular Value Cutoff
- **Method:** Remove singular values below threshold
- **Shows:** Smaller cutoff â†’ less divergence

### 3. Factor 2 Ablation: Test Projection
- **Method:** Project test data onto leading k modes only
- **Shows:** Fewer modes â†’ less divergence

### 4. Factor 3 Ablation: No Residuals
- **Method:** Use perfectly linear, noiseless labels
- **Shows:** No residuals â†’ NO divergence!

### 5. Singular Value Evolution
- **Shows:** Ïƒ_min reaches minimum at N = D

### 6. Adversarial Test Examples
- **Method:** Push test points along weakest mode
- **Shows:** Massively increased error

### 7. Adversarial Training (Poisoning)
- **Method:** Corrupt training labels along weakest mode
- **Shows:** Test error increases by 1-3 orders of magnitude!

## ğŸ“Š Example Usage

```julia
using .DoubleDescent

# Generate synthetic data
X, Y, Î²_star = generate_student_teacher_data(1000, 50, noise_std=0.2)

# Run double descent experiment
results = run_double_descent_experiment(X, Y, n_trials=10)

# Plot results
D = size(X, 2)
plot_double_descent(results, D, title="My Double Descent Experiment")

# Analyze singular values
U, S, V = analyze_singular_values(X)
println("Smallest singular value: ", minimum(S))
println("Condition number: ", maximum(S) / minimum(S))

# Run ablation: remove small singular values
results_ablation = ablation_singular_values(X, Y, cutoff_fractions=[0.0, 0.1, 0.2])

# Create adversarial test example
X_train = X[1:50, :]
Y_train = Y[1:50]
x_adv, info = create_adversarial_test(X_train, Y_train, magnitude=10.0)
```

## ğŸ¯ Key Insights

1. **Interpolation threshold is dangerous:**
   - At N = D, smallest singular value Ïƒ_min â†’ 0âº
   - Inverse 1/Ïƒ_min â†’ âˆ
   - Errors get massively amplified

2. **Overparameterization can help:**
   - Forces model to learn compressed representations
   - Projects onto row space: xÌ‚ = VVáµ€x
   - Often finds low-dimensional structure

3. **Three factors, one divergence:**
   - All three must be present
   - Remove any one â†’ safe!
   - Practical: use regularization, data augmentation, or stay away from N â‰ˆ D

4. **Adversarial examples are geometric:**
   - Test adversarial: push along weak modes (small Ïƒáµ£)
   - Train adversarial: corrupt residuals along weak modes
   - Both exploit the SVD structure

## ğŸ¨ Fun Examples from the PDF

The LaTeX document includes memorable analogies:

- ğŸ• **Pizza Delivery**: Predicting delivery times at interpolation threshold
- ğŸ’‘ **Dating Profiles**: Learning from too few dates
- ğŸ¥ **Pancakes**: How variance evolves with samples
- ğŸ—ºï¸ **GPS Navigation**: Extrapolating in poorly-covered directions
- ğŸ”® **Fortune Telling**: Predicting lottery numbers (spoiler: you can't)
- ğŸª **Party Guests**: Spanning a space with minimal samples
- ğŸ‘¥ **Shadows**: Overparameterized representation learning
- ğŸª **Tightrope Walk**: The danger of balancing at N = D

## ğŸ“š Mathematical Details

### Underparameterized (N > D)

**Solution:**
```
Î²Ì‚ = (Xáµ€X)â»Â¹Xáµ€Y = VSâ»Â¹Uáµ€Y
```

**Uses:** Second moment matrix Xáµ€X

### Overparameterized (N < D)

**Solution:**
```
Î²Ì‚ = Xáµ€(XXáµ€)â»Â¹Y = VSâ»Â¹Uáµ€Y
```

**Uses:** Gram matrix XXáµ€

**Both regimes share the same divergence term!**

### Bias-Variance Decomposition

**Underparameterized error:**
```
Å·_test - y_test* = x_testáµ€ VSâ»Â¹Uáµ€E
                   ï¸¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”ï¸¸
                   Divergence term
```

**Overparameterized error:**
```
Å·_test - y_test* = x_testáµ€ VSâ»Â¹Uáµ€E + x_testáµ€(I - VVáµ€)Î²*
                   ï¸¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”ï¸¸   ï¸¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”ï¸¸
                   Same divergence       Bias from projection
```

## ğŸ› ï¸ Extending the Code

### Add a New Dataset

```julia
function my_custom_dataset()
    # Load your data
    X = ... # N Ã— D matrix
    Y = ... # N vector

    # Standardize (recommended)
    X = (X .- mean(X, dims=1)) ./ std(X, dims=1)
    Y = (Y .- mean(Y)) ./ std(Y)

    return X, Y
end

# Use it
X, Y = my_custom_dataset()
results = run_double_descent_experiment(X, Y)
```

### Custom Experiment

```julia
# Manual control over train/test split
n_train = 30
X_train = X[1:n_train, :]
Y_train = Y[1:n_train]
X_test = X[n_train+1:end, :]
Y_test = Y[n_train+1:end]

# Fit model
model = LinearRegressionModel()
fit_model!(model, X_train, Y_train)

# Get detailed diagnostics
model, diag = fit_model_svd!(model, X_train, Y_train)

println("Regime: ", diag["regime"])
println("Min singular value: ", diag["min_singular_value"])
println("Condition number: ", diag["condition_number"])

# Analyze error components
error_components = compute_prediction_error_components(
    X_train, Y_train, X_test, Y_test, Î²_star
)

println("Divergence: ", error_components["divergence"])
```

## ğŸ“ References

1. **Original Blog Post:** [ICLR 2024 - Double Descent Demystified](https://iclr-blogposts.github.io/2024/blog/double-descent-demystified/)

2. **Paper:** Schaeffer, R., et al. (2023). "Double Descent Demystified: Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle." [arXiv:2303.14151](https://arxiv.org/abs/2303.14151)

3. **Related Work:**
   - Belkin et al. (2019). "Reconciling modern machine learning practice and the classical bias-variance trade-off." *PNAS*
   - Advani & Pennington (2020). "Understanding double descent requires a fine-grained bias-variance decomposition." *NeurIPS*
   - Bartlett et al. (2020). "Benign overfitting in linear regression." *PNAS*

## ğŸ¤ Contributing

This is an educational resource. Feel free to:
- Add new datasets
- Implement additional experiments
- Improve visualizations
- Add more funny examples to the PDF!

## ğŸ“„ License

Educational use. Based on publicly available research.

## â“ FAQ

**Q: Why Julia instead of Python?**
A: Julia is faster for matrix operations, has cleaner mathematical syntax, and is a joy for numerical computing. But the concepts translate directly to Python/NumPy!

**Q: Do I need to understand all the math?**
A: No! Start with the funny examples in the PDF. The math makes more sense once you have intuition.

**Q: How do I avoid double descent in practice?**
A:
1. Stay away from N â‰ˆ D (interpolation threshold)
2. Use regularization (ridge, lasso, elastic net)
3. Add more diverse data (increases Ïƒ_min)
4. Use early stopping
5. Ensemble methods

**Q: Does this apply to deep learning?**
A: Yes! The intuition extends to:
- Neural Tangent Kernel (NTK) regime
- Wide neural networks
- Kernel methods
- Autoencoders (superposition)

**Q: What if my test error doesn't show double descent?**
A: Check the three factors:
1. Are singular values small at N=D? (plot Ïƒ_min evolution)
2. Do test points vary in weak directions?
3. Are there residual errors? (try noiseless data)

If any factor is missing, no double descent!

## ğŸ“ Learning Path

1. **Start here:** Read the funny examples in Section 1-3 of the PDF
2. **Understand SVD:** Section 4 of the PDF
3. **Run experiments:** `julia run_experiments.jl`
4. **Read the math:** Sections 5-6 of the PDF with experiment results
5. **Deep dive:** Ablation studies and adversarial examples
6. **Extend:** Try your own datasets!

## ğŸ“¬ Questions?

The math is subtle but beautiful. The key insight:

> **Three factors, when all present, create a perfect storm at the interpolation threshold. Remove any one, and you're safe.**

Happy learning! ğŸ‰

---

*"The tightrope at N=D is scary. Either walk on the bridge (underparameterized) or jump into the cloud (overparameterized)!"*
â€” From the Tutorial
