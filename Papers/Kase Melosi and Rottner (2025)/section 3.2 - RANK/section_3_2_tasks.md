# Section 3.2: RANK Model with ZLB - Complete Task Checklist

**Paper Reference:** Kase, Melosi, and Rottner (2025) - "Estimating nonlinear heterogeneous agents models with neural networks"

**Goal:** Implement proof of concept 2 comparing neural network estimation method to state-of-the-art global methods for a RANK model with occasionally binding ZLB constraint.

**Total Estimated Time:** 2-3 weeks

---

## ðŸ“‹ Quick Navigation

- [Phase 1: Model Foundation (Tasks 1-5)](#phase-1-model-foundation)
- [Phase 2: Neural Network for Policy Functions (Tasks 6-10)](#phase-2-neural-network-for-policy-functions)
- [Phase 3: Particle Filter (Tasks 11-14)](#phase-3-particle-filter)
- [Phase 4: Bayesian Estimation (Tasks 15-18)](#phase-4-bayesian-estimation)
- [Phase 5: Comparison and Validation (Tasks 19-22)](#phase-5-comparison-and-validation)

---

## Phase 1: Model Foundation

### âœ… Task 1: Set Up Project Structure

**Estimated Time:** 30 minutes

**Description:** Create organized folder structure for Section 3.2 implementation.

**Deliverables:**
```
section_3_2_RANK/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ rank_model.py
â”œâ”€â”€ networks/
â”‚   â”œâ”€â”€ policy_nn.py
â”‚   â””â”€â”€ pf_nn.py
â”œâ”€â”€ estimation/
â”‚   â”œâ”€â”€ particle_filter.py
â”‚   â”œâ”€â”€ train_policy.py
â”‚   â”œâ”€â”€ train_pf.py
â”‚   â””â”€â”€ rwmh.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ priors.py
â”‚   â”œâ”€â”€ quasi_random.py
â”‚   â””â”€â”€ plotting.py
â”œâ”€â”€ data/
â”œâ”€â”€ results/
â”œâ”€â”€ figures/
â””â”€â”€ main.py
```

**How to Complete:**
1. Create directory structure
2. Create empty Python files with docstrings
3. Set up virtual environment: `conda create -n dsge-nn python=3.10`
4. Install dependencies:
   ```bash
   pip install torch torchvision numpy scipy pandas matplotlib seaborn tensorboard tqdm
   ```

**Success Criteria:**
- [ ] All folders created
- [ ] Virtual environment activated
- [ ] All dependencies installed
- [ ] Can import torch successfully

---

### âœ… Task 2: Define RANK Model Parameters

**Estimated Time:** 1 hour

**File:** `models/rank_model.py`

**Description:** Create parameter dataclass with calibrated and estimated parameters following Appendix C of the paper.

**Key Parameters:**

**Calibrated (Fixed):**
- $\beta = 0.9975$ (discount factor â†’ 4% annual interest)
- $\sigma = 1.0$ (risk aversion)
- $\eta = 0.72$ (inverse Frisch elasticity)
- $\epsilon = 10.0$ (elasticity of substitution)
- $\chi = 0.74$ (disutility of labor, calibrated for $H=1$)
- $\Pi = 1.0$ (inflation target)

**Estimated:**
- $\theta_\pi \in [1.5, 3.0]$ (Taylor rule inflation response)
- $\theta_y \in [0.1, 1.0]$ (Taylor rule output response)
- $\phi \in [50, 200]$ (Rotemberg adjustment cost)
- $\rho_\zeta \in [0.5, 0.95]$ (preference shock persistence)
- $\sigma_\zeta \in [0.01, 0.05]$ (preference shock std dev)

**Deliverables:**
- `RANKParameters` dataclass
- `get_bounds()` method for parameter bounds
- `to_dict()` method for vectorization

**Success Criteria:**
- [ ] All parameters defined with correct types
- [ ] Bounds properly specified
- [ ] Can create parameter instances
- [ ] Can convert to dict for batch operations

**Reference:** See paper Appendix C.1, Table 5

---

### âœ… Task 3: Implement Model Equilibrium Conditions

**Estimated Time:** 3-4 hours

**File:** `models/rank_model.py`

**Description:** Implement the 8 equilibrium conditions for RANK model with ZLB.

**Equations to Implement:**

1. **Euler Equation:**
   $$C_t^{-\sigma} \exp(\zeta_t) = \beta E_t[C_{t+1}^{-\sigma} \exp(\zeta_{t+1}) R_t / \Pi_{t+1}]$$

2. **New Keynesian Phillips Curve (NKPC):**
   $$\phi\left(\frac{\Pi_t}{\Pi} - 1\right)\frac{\Pi_t}{\Pi} = (1-\epsilon) + \epsilon MC_t + \phi E_t\left[\frac{R_t}{\Pi_{t+1}}\left(\frac{\Pi_{t+1}}{\Pi} - 1\right)\frac{\Pi_{t+1}}{\Pi}\frac{Y_{t+1}}{Y_t}\right]$$

3. **Taylor Rule with ZLB:**
   $$R_t = \max\left[1, R \left(\frac{\Pi_t}{\Pi}\right)^{\theta_\pi} \left(\frac{Y_t}{Y}\right)^{\theta_y}\right]$$

4. **Market Clearing:** $Y_t = C_t$

5. **Production:** $Y_t = N_t$

6. **Labor Supply:** $\chi H_t^\eta = W_t C_t^{-\sigma}$

7. **Marginal Cost:** $MC_t = W_t$

8. **Preference Shock:** $\zeta_t = \rho_\zeta \zeta_{t-1} + \epsilon_t^\zeta$

**Deliverables:**
- `RANKModel` class
- `_compute_steady_state()` method
- `equilibrium_residuals()` method returning $[\text{Euler residual}, \text{NKPC residual}]$

**Success Criteria:**
- [ ] Steady state computes correctly
- [ ] Residuals are zero at steady state
- [ ] Can handle batch inputs (parameters as pseudo-states)
- [ ] ZLB constraint properly implemented

**Reference:** Paper Appendix C.1

---

### âœ… Task 4: Implement State Transition (Shock Process)

**Estimated Time:** 30 minutes

**File:** `models/rank_model.py`

**Description:** Implement AR(1) process for preference shock.

**Process:**
$$\zeta_t = \rho_\zeta \zeta_{t-1} + \epsilon_t^\zeta, \quad \epsilon_t^\zeta \sim N(0, \sigma_\zeta^2)$$

**Deliverables:**
- `simulate_shocks(T, params, seed)` method
- Proper random seed handling for reproducibility

**Success Criteria:**
- [ ] Shocks follow AR(1) process
- [ ] Can specify different parameter values
- [ ] Reproducible with same seed
- [ ] Returns tensor of length T

---

### âœ… Task 5: Create Data Simulation Function

**Estimated Time:** 1-2 hours

**File:** `models/rank_model.py`

**Description:** Simulate observable time series using trained policy functions.

**Observables:**
- Output: $Y_t$
- Inflation: $\Pi_t$
- Interest rate: $R_t$

**Deliverables:**
- `simulate_data(T, params, policy_nn, seed)` method
- Returns dict with `{'Y': ..., 'Pi': ..., 'R': ..., 'zeta': ...}`

**Success Criteria:**
- [ ] Can simulate T periods forward
- [ ] Uses policy network to get controls
- [ ] Applies ZLB constraint correctly
- [ ] Reproducible with same seed

---

## Phase 2: Neural Network for Policy Functions

### âœ… Task 6: Define Policy Function Neural Network Architecture

**Estimated Time:** 1 hour

**File:** `networks/policy_nn.py`

**Description:** Create neural network to approximate policy functions as functions of states AND parameters.

**Architecture (from Paper Section 3.2):**
- **Input:** $(6)$ = $[\zeta_t, \theta_\pi, \theta_y, \phi, \rho_\zeta, \sigma_\zeta]$
- **Hidden layers:** 5 layers, 128 neurons each
- **Activation:** SiLU (Sigmoid Linear Unit)
- **Output:** $(2)$ = $[N_t, \Pi_t]$ (labor, inflation)

**Deliverables:**
- `PolicyNetwork` class inheriting from `nn.Module`
- Xavier initialization for weights
- Forward pass implementation

**Success Criteria:**
- [ ] Correct architecture (5 hidden layers, 128 neurons)
- [ ] SiLU activation functions
- [ ] Can process batched inputs
- [ ] Output shape is (batch, 2)

**Reference:** Paper Section 3.2, network specifications

---

### âœ… Task 7: Implement Quasi-Random Parameter Sampling

**Estimated Time:** 1 hour

**File:** `utils/quasi_random.py`

**Description:** Generate low-discrepancy sequences for parameter sampling using Sobol sequences.

**Why Sobol?** Better coverage of parameter space than uniform random sampling.

**Deliverables:**
- `quasi_random_sample(bounds, n_samples, seed)` function
- Uses `scipy.stats.qmc.Sobol`
- Scales from $[0,1]^d$ to parameter bounds

**Success Criteria:**
- [ ] Generates correct number of samples
- [ ] Samples within specified bounds
- [ ] Returns both samples and parameter names
- [ ] Reproducible with same seed

**Reference:** Paper mentions quasi-random draws

---

### âœ… Task 8: Implement ZLB Constraint Scheduling

**Estimated Time:** 1 hour

**File:** `networks/policy_nn.py`

**Description:** Gradually introduce ZLB constraint during training (curriculum learning).

**Approach (from Paper Figure 11):**
Start with soft constraint, gradually make harder:
$$R_t = \max[R_t^N, 1] + \alpha^{ZLB} \cdot \min[R_t^N - 1, 0]$$

where $\alpha^{ZLB}$ goes from 0 â†’ 1 over iterations 5000-10000.

**Deliverables:**
- `ZLBScheduler` class
- `get_alpha(iteration)` method
- `apply_zlb(R_notional, iteration)` method

**Success Criteria:**
- [ ] $\alpha = 0$ before iteration 5000
- [ ] $\alpha = 1$ after iteration 10000
- [ ] Linear interpolation in between
- [ ] Can apply to batched interest rates

**Reference:** Paper Section 3.2, Figure 11

---

### âœ… Task 9: Implement Policy Function Training Loop

**Estimated Time:** 2-3 hours

**File:** `estimation/train_policy.py`

**Description:** Train neural network to satisfy equilibrium conditions across state-parameter space.

**Loss Function:**
$$\mathcal{L} = \mathbb{E}\left[(\text{Euler residual})^2 + (\text{NKPC residual})^2\right]$$

**Training Details:**
- **Iterations:** 100,000
- **Batch size:** 100
- **Optimizer:** AdamW
- **Learning rate:** Cosine annealing from $1 \times 10^{-4}$ to $1 \times 10^{-7}$
- **ZLB introduction:** Iterations 5000-10000

**Deliverables:**
- `train_policy_network()` function
- TensorBoard logging
- Model checkpointing
- Training history

**Success Criteria:**
- [ ] Loss decreases over training
- [ ] Final MSE < $10^{-5}$
- [ ] TensorBoard logs created
- [ ] Model saved to disk

**Reference:** Paper Section 3.2, Figure 11

---

### âœ… Task 10: Validate Policy Function Accuracy

**Estimated Time:** 1 hour

**File:** `estimation/train_policy.py`

**Description:** Verify trained policy functions produce small equilibrium residuals.

**Tests:**
1. Evaluate residuals on test set
2. Check policy function smoothness
3. Verify ZLB binds appropriately

**Deliverables:**
- `validate_policy_network()` function
- Prints max/mean residuals
- Reports output ranges

**Success Criteria:**
- [ ] Maximum residual < $10^{-4}$
- [ ] Mean residual < $10^{-5}$
- [ ] Policy functions are smooth
- [ ] Validation report printed

---

## Phase 3: Particle Filter

### âœ… Task 11: Implement Standard Bootstrap Particle Filter

**Estimated Time:** 3-4 hours

**File:** `estimation/particle_filter.py`

**Description:** Implement bootstrap particle filter for likelihood evaluation.

**Algorithm (Herbst & Schorfheide 2015):**
1. Initialize particles from stationary distribution
2. For each time $t$:
   - Propagate particles: $\zeta_t^{(i)} = \rho_\zeta \zeta_{t-1}^{(i)} + \epsilon_t^{(i)}$
   - Compute weights: $w_t^{(i)} \propto p(y_t | \zeta_t^{(i)}, \theta)$
   - Normalize weights
   - Resample if ESS low
3. Return log-likelihood: $\sum_{t=1}^T \log \sum_i w_t^{(i)}$

**Deliverables:**
- `ParticleFilter` class
- `initialize_particles()` method
- `propagate_particles()` method
- `observation_density()` method (uses policy NN!)
- `resample()` method (systematic resampling)
- `filter()` method (main algorithm)

**Success Criteria:**
- [ ] Returns log-likelihood value
- [ ] Can handle different parameter values
- [ ] ESS monitored and resampling triggered
- [ ] Uses policy network for predictions

**Reference:** Herbst & Schorfheide (2015) textbook

---

### âœ… Task 12: Test Particle Filter on Simulated Data

**Estimated Time:** 1 hour

**File:** `estimation/particle_filter.py`

**Description:** Validate particle filter implementation.

**Test:**
1. Simulate data with known true parameters
2. Run particle filter with true parameters
3. Check log-likelihood is reasonable (not -âˆž)

**Deliverables:**
- `test_particle_filter()` function
- Prints log-likelihood and per-observation value

**Success Criteria:**
- [ ] Filter runs without errors
- [ ] Log-likelihood is finite
- [ ] Per-observation likelihood ~ -5 to 0

---

### âœ… Task 13: Generate Training Data for NN Particle Filter

**Estimated Time:** 4-8 hours (computational)

**File:** `estimation/train_pf.py`

**Description:** Generate (parameters, log-likelihood) pairs for training NN particle filter.

**Process:**
- Draw 10,000 parameter combinations (quasi-random)
- For each:
  1. Simulate data (100 periods)
  2. Run standard particle filter (100 particles)
  3. Record log-likelihood

**Deliverables:**
- `generate_pf_training_data()` function
- Saves `(param_samples, log_likelihoods)` to disk
- Progress bar with tqdm

**Success Criteria:**
- [ ] 10,000 samples generated
- [ ] No NaN or -âˆž likelihoods
- [ ] Data saved successfully
- [ ] Takes ~4-8 hours on GPU

**Note:** This is the computational bottleneck! Use 100 particles (deliberately noisy) as in paper.

**Reference:** Paper mentions 10,000 draws for training

---

### âœ… Task 14: Train NN Particle Filter

**Estimated Time:** 2-3 hours

**File:** `estimation/train_pf.py`, `networks/pf_nn.py`

**Description:** Train neural network to approximate likelihood function, smoothing out particle filter noise.

**Architecture:**
- **Input:** $(5)$ = $[\theta_\pi, \theta_y, \phi, \rho_\zeta, \sigma_\zeta]$
- **Hidden layers:** 4 layers, 128 neurons each
- **Activation:** CELU
- **Output:** $(1)$ = log-likelihood

**Training:**
- **Epochs:** 20,000
- **Batch size:** 100
- **Train/val split:** 80/20
- **Optimizer:** AdamW
- **Learning rate:** Cosine annealing $2 \times 10^{-4}$ â†’ $1 \times 10^{-7}$

**Deliverables:**
- `ParticleFilterNN` class
- `train_pf_network()` function
- Training/validation loss curves
- Model saved to disk

**Success Criteria:**
- [ ] Validation loss decreases
- [ ] No overfitting (train/val gap small)
- [ ] NN smooths particle filter noise (as in Figure 5)
- [ ] Model saved successfully

**Reference:** Paper Section 3.1, Figure 5

---

## Phase 4: Bayesian Estimation

### âœ… Task 15: Implement Prior Distributions

**Estimated Time:** 1-2 hours

**File:** `utils/priors.py`

**Description:** Define truncated normal priors for all estimated parameters.

**Prior Specification:**
All parameters use truncated normal distributions with:
- Mean at middle of range
- Standard deviation allowing exploration
- Hard bounds from parameter ranges

**Deliverables:**
- `TruncatedNormalPrior` class
- `RANKPriors` class with all 5 parameter priors
- `log_prior()` method
- `sample_prior()` method

**Success Criteria:**
- [ ] All 5 parameters have priors
- [ ] Can evaluate log prior density
- [ ] Can sample from prior
- [ ] Samples respect bounds

**Reference:** Paper Appendix C, Table 5

---

### âœ… Task 16: Implement RWMH Algorithm - Initialization

**Estimated Time:** 2 hours

**File:** `estimation/rwmh.py`

**Description:** Implement Step 1 of Appendix A algorithm.

**Algorithm:**
1. Draw 1,000 candidates from prior
2. Evaluate log posterior for each
3. Keep top 10% to construct covariance matrix
4. Set mode as starting point

**Deliverables:**
- `RWMH` class
- `log_posterior()` method (uses NN particle filter!)
- `initialize()` method

**Success Criteria:**
- [ ] Finds mode successfully
- [ ] Covariance matrix is positive definite
- [ ] Progress bar shows evaluations

**Reference:** Paper Appendix A, Algorithm step 1

---

### âœ… Task 17: Implement RWMH Algorithm - Burn-in

**Estimated Time:** 1 hour

**File:** `estimation/rwmh.py`

**Description:** Implement Step 2 of Appendix A algorithm.

**Algorithm:**
1. Run 10,000 RWMH iterations
2. Tune scaling parameter $c$ for 20-40% acceptance
3. Update covariance matrix from last 75% of draws

**Deliverables:**
- `run_burn_in()` method
- Adaptive tuning of $c$
- Returns updated covariance

**Success Criteria:**
- [ ] 10,000 iterations complete
- [ ] Acceptance rate in target range
- [ ] Covariance updated
- [ ] Progress bar with acceptance rate

**Reference:** Paper Appendix A, Algorithm step 2

---

### âœ… Task 18: Implement RWMH Algorithm - Main Sampling

**Estimated Time:** 1 hour

**File:** `estimation/rwmh.py`

**Description:** Implement Step 3 of Appendix A algorithm.

**Algorithm:**
1. Run 50,000 RWMH iterations
2. Save all draws
3. Monitor acceptance rate

**Deliverables:**
- `run_sampling()` method
- `estimate()` method (full pipeline)
- Saves results to disk

**Success Criteria:**
- [ ] 50,000 draws generated
- [ ] Acceptance rate 20-40%
- [ ] Results saved as .npz file
- [ ] Takes ~1-2 hours

**Reference:** Paper Appendix A, Algorithm step 3

---

## Phase 5: Comparison and Validation

### âœ… Task 19: (Optional) Implement Global Method for Comparison

**Estimated Time:** 4-8 hours (optional)

**File:** `estimation/global_method.py`

**Description:** Implement Richter et al. (2014) global solution method for comparison.

**Note:** This is OPTIONAL. The main goal of Section 3.2 is to validate the NN method. You can skip this and just use NN results.

**If implementing:**
- Time iteration with piecewise linear policy functions
- Particle filter at each RWMH draw
- Very slow (that's why NN method is better!)

**Deliverables:**
- Global solution code
- Comparison to NN method

**Success Criteria:**
- [ ] Global method runs
- [ ] Results similar to NN method

**Reference:** Richter et al. (2014), Herbst & Schorfheide (2015)

---

### âœ… Task 20: Create Comparison Plots (Figure 6 Replication)

**Estimated Time:** 2 hours

**File:** `utils/plotting.py`

**Description:** Create plots comparing posterior distributions from NN method (and optionally global method).

**Plots:**
1. Trace plots (check mixing)
2. Posterior histograms
3. Overlay true parameter values
4. 90% credible intervals

**Deliverables:**
- `plot_posterior_diagnostics()` function
- `compute_posterior_moments()` function
- Saves high-quality figures

**Success Criteria:**
- [ ] All 5 parameters plotted
- [ ] True values shown
- [ ] Posteriors look reasonable
- [ ] No convergence issues

**Reference:** Paper Figure 6

---

### âœ… Task 21: Create Likelihood Surface Plot (Figure 5 Replication)

**Estimated Time:** 1 hour

**File:** `utils/plotting.py`

**Description:** Show how NN particle filter smooths particle filter noise.

**Plot:**
- X-axis: One parameter (e.g., $\theta_\pi$)
- Y-axis: Log-likelihood
- Blue line: NN particle filter (smooth)
- Orange dots: Standard particle filter (noisy)

**Deliverables:**
- `plot_likelihood_surface()` function
- Figure matching paper's Figure 5

**Success Criteria:**
- [ ] Smooth NN likelihood curve
- [ ] Shows NN cuts through PF noise
- [ ] True parameter value marked

**Reference:** Paper Figure 5

---

### âœ… Task 22: Write Section 3.2 Summary Report

**Estimated Time:** 2-3 hours

**File:** `reports/section_3_2_results.md` or Jupyter notebook

**Description:** Comprehensive analysis documenting all results.

**Contents:**
1. **Model Specification**
   - RANK equations
   - Parameter values
   - ZLB constraint

2. **Policy Function Training**
   - Convergence plots
   - Final MSE
   - Validation metrics

3. **NN Particle Filter Training**
   - Training/validation curves
   - Overfitting analysis
   - Likelihood surface plots

4. **Posterior Estimation**
   - RWMH diagnostics
   - Posterior moments table
   - Comparison to true values

5. **Key Findings**
   - Parameter recovery accuracy
   - Computation time
   - Comparison to global method (if applicable)

**Deliverables:**
- Complete results document
- All figures embedded
- Statistical tables
- Interpretation of results

**Success Criteria:**
- [ ] All sections complete
- [ ] Figures clear and labeled
- [ ] Results match paper's findings
- [ ] Code fully documented

---

## ðŸ“Š Summary Statistics

### Task Distribution by Phase

| Phase | Tasks | Est. Time | % of Total |
|-------|-------|-----------|------------|
| 1. Foundation | 1-5 | 6-8 hours | 20% |
| 2. Policy NNs | 6-10 | 8-12 hours | 30% |
| 3. Particle Filter | 11-14 | 10-18 hours | 35% |
| 4. Estimation | 15-18 | 5-7 hours | 15% |
| 5. Validation | 19-22 | 5-9 hours | 0% |
| **TOTAL** | **22** | **34-54 hours** | **100%** |

### Computational Requirements

| Task | CPU Time | GPU Time | Notes |
|------|----------|----------|-------|
| Train Policy NN | ~20 hours | ~2 hours | 100K iterations |
| Generate PF Data | ~12 hours | ~4 hours | 10K samples Ã— 100 periods |
| Train PF NN | ~4 hours | ~1 hour | 20K epochs |
| Run RWMH | ~2 hours | ~1 hour | 50K draws |
| **TOTAL** | **~38 hours** | **~8 hours** | GPU highly recommended |

---

## ðŸŽ¯ Milestones

### Week 1: Model Foundation + Policy Training
- [ ] **Day 1-2:** Tasks 1-5 (Model setup)
- [ ] **Day 3-5:** Tasks 6-10 (Train policy network)
- [ ] **Milestone:** Trained policy network with small residuals

### Week 2: Particle Filter + NN Training
- [ ] **Day 1-3:** Tasks 11-13 (Particle filter + data generation)
- [ ] **Day 4-5:** Task 14 (Train NN particle filter)
- [ ] **Milestone:** Smooth likelihood approximation

### Week 3: Estimation + Analysis
- [ ] **Day 1-2:** Tasks 15-18 (RWMH estimation)
- [ ] **Day 3-4:** Tasks 20-21 (Plots and analysis)
- [ ] **Day 5:** Task 22 (Final report)
- [ ] **Milestone:** Complete Section 3.2 implementation

---

## ðŸš€ Quick Start Guide

### Option 1: Run Full Pipeline
```bash
# After setting up environment
python section_3_2_complete.py --task all

# This will:
# 1. Train policy network (~2 hours on GPU)
# 2. Generate PF training data (~4 hours)
# 3. Train NN particle filter (~1 hour)
# 4. Run RWMH estimation (~1 hour)
# 5. Generate all plots and analysis
```

### Option 2: Run Tasks Individually
```bash
# Train policy network only
python section_3_2_complete.py --task train_policy

# Generate particle filter training data
python section_3_2_complete.py --task generate_pf_data

# Train NN particle filter
python section_3_2_complete.py --task train_pf

# Run estimation
python section_3_2_complete.py --task estimate
```

### Option 3: Interactive Development
```python
# In Jupyter notebook or IPython
from section_3_2_complete import *

# Initialize model
params = RANKParameters()
model = RANKModel(params)

# Train policy network
policy_nn = PolicyNetwork()
policy_nn, history = train_policy_network(model, policy_nn)

# ... continue with other tasks
```

---

## ðŸ” Debugging Tips

### Common Issues

**Issue 1: Policy network not converging**
- Check learning rate schedule
- Verify equilibrium conditions
- Ensure ZLB scheduling is correct
- Try longer training (200K iterations)

**Issue 2: Particle filter returns -âˆž**
- Check measurement error variance (try 1-5%)
- Verify policy network is loaded correctly
- Ensure particles initialized from stationary distribution
- Check for numerical overflow

**Issue 3: RWMH acceptance rate too low/high**
- Adjust scaling parameter $c$
- Increase burn-in iterations
- Check proposal covariance matrix
- Verify prior bounds are reasonable

**Issue 4: NN particle filter overfitting**
- Check train/val split (80/20)
- Monitor validation loss
- Early stopping if val loss increases
- Reduce model capacity if needed

---

## ðŸ“š Key References

1. **Main Paper:** Kase, Melosi, Rottner (2025) - Sections 3.2, Appendix C
2. **Particle Filtering:** Herbst & Schorfheide (2015) - Chapters 9-10
3. **Global Methods:** Richter et al. (2014)
4. **RWMH:** Chib & Greenberg (1995)
5. **Neural Networks:** Goodfellow et al. (2016) - Deep Learning textbook

---

## âœ… Final Checklist

Before declaring Section 3.2 complete, verify:

### Code Quality
- [ ] All functions have docstrings
- [ ] Code follows PEP 8 style
- [ ] No hardcoded paths
- [ ] Random seeds set for reproducibility
- [ ] All imports organized

### Results
- [ ] Policy network residuals < 10^-5
- [ ] NN particle filter smooths PF noise
- [ ] RWMH acceptance rate 20-40%
- [ ] Posterior medians close to true values
- [ ] All 5 parameters' 90% CI contain true values

### Documentation
- [ ] README.md with setup instructions
- [ ] All figures saved with descriptive names
- [ ] Results summary document complete
- [ ] Code comments explain key steps

### Reproducibility
- [ ] requirements.txt or environment.yml
- [ ] All random seeds documented
- [ ] Results can be reproduced from scratch
- [ ] Intermediate outputs saved

---

## ðŸŽ“ Learning Outcomes

By completing Section 3.2, you will have:

1. âœ… Implemented a nonlinear DSGE model (RANK with ZLB)
2. âœ… Trained neural networks to approximate policy functions
3. âœ… Implemented bootstrap particle filter from scratch
4. âœ… Trained NN to approximate likelihood function
5. âœ… Implemented full Bayesian estimation with RWMH
6. âœ… Validated against true parameters
7. âœ… Created publication-quality figures
8. âœ… Gained deep understanding of:
   - Neural network training in PyTorch
   - Sequential Monte Carlo methods
   - Bayesian inference for DSGE models
   - Curriculum learning (ZLB scheduling)
   - Quasi-random sampling techniques

---

## ðŸ†˜ Getting Help

If you get stuck:

1. **Check the paper:** Appendix C has all model details
2. **Review the code:** `section_3_2_complete.py` has full implementation
3. **Common errors:** See debugging tips above
4. **Computational resources:** GPU highly recommended

**Good luck!** ðŸš€

This is a substantial project that will give you a complete understanding of the neural network estimation methodology for nonlinear DSGE models.
